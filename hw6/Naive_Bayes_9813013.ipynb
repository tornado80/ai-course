{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes 9813013.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Please run these cells"
      ],
      "metadata": {
        "id": "iTlmEckSj-wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown --id 1fdCV3sin_ZZ9BhZt6BhquPtZ4IQ8UY96\n",
        "! gdown --id 11yepXZ4geifWW1kJ34sHdF94tWl0Y8M3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUW3zus6PGlX",
        "outputId": "a4222f52-3d4e-405b-85d3-9df44efbaeed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fdCV3sin_ZZ9BhZt6BhquPtZ4IQ8UY96\n",
            "To: /content/emails.rar\n",
            "100% 1.13M/1.13M [00:00<00:00, 101MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11yepXZ4geifWW1kJ34sHdF94tWl0Y8M3\n",
            "To: /content/stop-words.txt\n",
            "100% 14.3k/14.3k [00:00<00:00, 17.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! apt install unrar"
      ],
      "metadata": {
        "id": "Smeg8g2VK4Xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05530bf-efeb-4053-f684-3d2a32bbeeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:5.5.8-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ./dataset"
      ],
      "metadata": {
        "id": "YCahfeLKLveY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f999f4-a198-4b38-9cad-c48837630283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./dataset’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unrar x /content/emails.rar /content/dataset > /dev/null"
      ],
      "metadata": {
        "id": "F5dwYVwLLHuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e452ae8f-ff71-4a13-a7ad-433e3b826fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Would you like to replace the existing file /content/dataset/spamtraining/spamtraining (1).txt\n",
            " 21865 bytes, modified on 2015-10-22 21:20\n",
            "with a new one\n",
            " 21865 bytes, modified on 2015-10-22 21:20\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit n\n",
            "\n",
            "\n",
            "Would you like to replace the existing file /content/dataset/spamtraining/spamtraining (10).txt\n",
            "  4596 bytes, modified on 2015-10-22 21:20\n",
            "with a new one\n",
            "  4596 bytes, modified on 2015-10-22 21:20\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit N\n",
            "\n",
            "\n",
            "Would you like to replace the existing file /content/dataset/spamtraining/spamtraining (100).txt\n",
            "   969 bytes, modified on 2015-10-22 21:20\n",
            "with a new one\n",
            "   969 bytes, modified on 2015-10-22 21:20\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit Q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hazm -qq"
      ],
      "metadata": {
        "id": "xJWVny4WQuPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import"
      ],
      "metadata": {
        "id": "hsXa1yhZLwRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "from re import findall\n",
        "from hazm import Stemmer, word_tokenize\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "from glob import glob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import time\n",
        "\n",
        "\"\"\"Countvectorzer initialize\"\"\"\n",
        "vect = CountVectorizer()\n",
        "is_file = True\n",
        "\n",
        "\"\"\"stemmer from hazm\"\"\"\n",
        "stemmer = Stemmer()\n",
        "stopwords = set(open('stop-words.txt', encoding='utf8').read().splitlines())\n",
        "hamtrains = glob('dataset/hamtraining/hamtraining*.txt')\n",
        "spamtrains = glob('dataset/spamtraining/spamtraining*.txt')\n",
        "\n",
        "hamtests = glob('dataset/hamtesting/hamtesting*.txt')\n",
        "spamtests = glob('dataset/spamtesting/spamtesting*.txt')\n",
        "\n",
        "ham_spam_test = hamtests + spamtests\n",
        "ham_spam_train = hamtrains + spamtrains\n",
        "\n",
        "y_train = [False for i in range(300)]\n",
        "y_spam_train = [True for i in range(300)]\n",
        "y_train.extend(y_spam_train)\n",
        "\n",
        "\n",
        "y_test = [False for i in range(200)]\n",
        "y_spam_test = [True for i in range(200)]\n",
        "y_test.extend(y_spam_test)"
      ],
      "metadata": {
        "id": "_fADt8EfOQas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "RkAXHzrkL1S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class const:\n",
        "    farsi = ('ب', 'س', 'ش', 'ل', 'ت', 'ن', 'م', 'گ', 'ظ', 'ط', 'ز',\n",
        "             'ر', 'ژ', 'ذ', 'د', 'پ', 'چ', 'ج', 'ح', 'ع', \n",
        "             'خ', 'غ', 'ف', 'ق', 'ث', 'ص', 'ض','\\u0020', '\\u060c','؟', '!', '?', '.', ':','\\n','_')\n",
        "\n",
        "    alef = ('ا', 'آ', 'ء', 'أ', 'إ')\n",
        "    vav = ('و', 'ؤ')\n",
        "    heh = ('ه', 'ة', 'ە')\n",
        "    yah = ('ی', 'ي', 'ئ', 'ى')\n",
        "    kaf = ('ک', 'ك')\n",
        "    punc = ('_', '-')\n",
        "\n",
        "def persian_char(char):\n",
        "    if char in const.farsi:\n",
        "        return char\n",
        "    if char in const.alef:\n",
        "        return const.alef[0]\n",
        "    if char in const.vav:\n",
        "        return const.vav[0]\n",
        "    if char in const.heh:\n",
        "        return const.heh[0]\n",
        "    if char in const.yah:\n",
        "        return const.yah[0]\n",
        "    if char in const.kaf:\n",
        "        return const.kaf[0]\n",
        "    if char in const.punc:\n",
        "      return ' '\n",
        "    return ''\n",
        "\n",
        "def pre_process(path):\n",
        "    if is_file == False:\n",
        "        text = path\n",
        "    else:\n",
        "        text = open(path, encoding='utf8').read()\n",
        "\n",
        "#     urls = len(findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', text))\n",
        "    \n",
        "    map_test = map(persian_char, text)\n",
        "    sentence = ''.join(map_test)\n",
        "    word_tokens = word_tokenize(sentence)\n",
        "    filtered_sentence = [w for w in word_tokens if w not in stopwords] #+ ['url' for i in range(urls)]\n",
        "    filtered_sentence = ' '.join(filtered_sentence)\n",
        "    return filtered_sentence"
      ],
      "metadata": {
        "id": "TLo8Z3tjNY6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature(data):\n",
        "    if is_file == False:\n",
        "        feature_list = [pre_process(data)]\n",
        "    else:    \n",
        "        map_loop = map(pre_process, data)\n",
        "        feature_list = list(map_loop)\n",
        "    return feature_list"
      ],
      "metadata": {
        "id": "eq6fmbKJNAON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(feature_list):\n",
        "    X_dtm = vect.fit_transform(feature_list)\n",
        "    X_dtm = X_dtm.toarray()\n",
        "    return X_dtm"
      ],
      "metadata": {
        "id": "f4W7WDyHSIqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_selection(k,X_dtm):\n",
        "    chi2_features = SelectKBest(chi2, k=k)\n",
        "    X_kbest_features = chi2_features.fit_transform(X_dtm, y_train)\n",
        "    return X_kbest_features, chi2_features"
      ],
      "metadata": {
        "id": "EctJShjtRrtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(data,chi2_features):\n",
        "    tokens = feature(data)\n",
        "    x0 = vect.transform(tokens).toarray()\n",
        "    cx = chi2_features.transform(x0)\n",
        "    return cx"
      ],
      "metadata": {
        "id": "kW63H6H2M081"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_dtm = vectorize(feature(ham_spam_train))\n",
        "x_train, chi2_features = feature_selection(700,X_dtm)"
      ],
      "metadata": {
        "id": "QG_JW7OCRv33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Just for Intuition"
      ],
      "metadata": {
        "id": "So9qhaNiL6k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KVb5YkTRxbH",
        "outputId": "b935df00-b456-49fe-c604-9e8eecb5ee88",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Just for test and compare\"\"\"\n",
        "x_test = transform(ham_spam_test, chi2_features)\n",
        "predict_val = mnb.predict(x_test)"
      ],
      "metadata": {
        "id": "do9ij0utRzKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Show metrics and score!\"\"\"\n",
        "print(classification_report(y_test, predict_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqVRo_9cR1UB",
        "outputId": "d8c6259b-30c8-4ad7-d6ea-84d474801c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.93      0.97      0.95       200\n",
            "        True       0.97      0.93      0.95       200\n",
            "\n",
            "    accuracy                           0.95       400\n",
            "   macro avg       0.95      0.95      0.95       400\n",
            "weighted avg       0.95      0.95      0.95       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive bayes model"
      ],
      "metadata": {
        "id": "4T_yx4pSNBbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 dir=\"rtl\"> ویدیو بسیار مفید برای ساخت مدل<h2>\n",
        "<h2><a href=\"https://www.youtube.com/watch?v=O2L2Uv9pdDA\">Naive Bayes, Clearly Explained!!!</a><h2>"
      ],
      "metadata": {
        "id": "PgA5NkyZViH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "متغیر‌هایی که در مرحله تست و ترین مدل مورد استفاده قرار میگیرند.\n",
        "\n",
        "x_train, x_test, y_train, y_test\n",
        "\n",
        "خرجی تابع\n",
        "transform \n",
        "وکتورایز شده هر یک از سمپل‌های موجود در دیتاست تست است.\n",
        "به طوری که تعداد اعضای \n",
        "test_data\n",
        "برابر با 400 نمونه متنی است که برای تست کردن مدل مورد استفاده قرار میگیرد.\n",
        "هر یک از سمپل‌ها دارای وکتوری به طول 700 میباشد، که تعداد فیچرهای(کلمات) برتر انتخاب شده توسط تابع SelectKBest \n",
        "اند.\n",
        "\n",
        "در این وکتور 700 بعدی مقدار هر یک از اندیس‌ها نشان دهنده تکرار آن کلمه در آن سمپل خاص هست.\n",
        "\n",
        "راهنمایی ساخت جدول احتمالات:\n",
        "شما با کمک تکرار کلمات در هر سمپل و اینکه هر کلمه در کل نمونه‌های \n",
        "spam\n",
        "یا\n",
        "ham\n",
        "چند بار تکرار شده است میتوانید احتمال رخدادن هر کلمه را در هر دسته بدست آورید\n",
        "که در نهایت با ضرب احتمالات بدست آمده تعیین میکنید که ایمیل \n",
        "spam\n",
        "است یا نه.\n",
        "\n",
        "به این شکل که بعد از ضرب احتمالات در هم دیگر؛ اگر مقدار احتمال بدست آمده با توجه به جدول احتمالاتی \n",
        "spam \n",
        "بیشتر از جدول \n",
        "ham(nonspam)\n",
        "بود،\n",
        "آنگاه آن ایمیل spam \n",
        "است.\n",
        "<h3>"
      ],
      "metadata": {
        "id": "8mkQImpgGIUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = transform(ham_spam_test, chi2_features)"
      ],
      "metadata": {
        "id": "5PWMEKHsNhaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vVys7IZJLUJ",
        "outputId": "20cc1fb4-348b-496d-91ba-e80afc29ef0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\"> \n",
        "شما باید با کمک این فیچر‌ها مدل \n",
        "naive bayes \n",
        "را برای تشخیص ایمیل‌های \n",
        "spam, nonspam\n",
        "آماده سازی کنید.\n",
        "خروجی کار شما باید لیبل‌های باشد که تعیین میکند،\n",
        "ایمیل \n",
        "spam هست\n",
        "یا نه.\n",
        "\n",
        "<h3>"
      ],
      "metadata": {
        "id": "CItWMrA3JqAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Please implement your naive bayse model in here.<h1>"
      ],
      "metadata": {
        "id": "6yApjdXwLtOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "#Build your naive bayes classifier!\n",
        "from functools import reduce\n",
        "from math import log\n",
        "log_probability_of_spam = log(0.5) #reduce(lambda x, y: x + 1 if y else x, y_train, 0) / len(y_train) # spam is True\n",
        "log_probability_of_ham = log(1 - log_probability_of_spam)\n",
        "spam_train = []\n",
        "ham_train = []\n",
        "total_occurrences_in_spams = total_occurrences_in_hams = 700 # 700 features for laplacian smoothing in denominator\n",
        "for i, sample in enumerate(x_train):\n",
        "    if y_train[i]:\n",
        "        total_occurrences_in_spams += sum(sample)\n",
        "        spam_train.append(sample)\n",
        "    else:\n",
        "        total_occurrences_in_hams += sum(sample)\n",
        "        ham_train.append(sample)\n",
        "log_conditional_features_given_spam = []\n",
        "log_conditional_features_given_ham = []\n",
        "for feature in range(700):\n",
        "    p = log(reduce(lambda x, y: x + y[feature], spam_train, 1) / total_occurrences_in_spams) # 1 is for laplacian smoothing in numerator\n",
        "    q = log(reduce(lambda x, y: x + y[feature], ham_train, 1) / total_occurrences_in_hams)\n",
        "    log_conditional_features_given_spam.append(p)\n",
        "    log_conditional_features_given_ham.append(q)\n",
        "def predict(sample):\n",
        "    s = log_probability_of_spam\n",
        "    h = log_probability_of_ham\n",
        "    for feature in range(700):\n",
        "        s += log_conditional_features_given_spam[feature] * sample[feature]\n",
        "        h += log_conditional_features_given_ham[feature] * sample[feature]\n",
        "    return s > h\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "VDLFHZYZLWdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### START CODE HERE ###\n",
        "y_predict = [predict(sample) for sample in test_data]\n",
        "### END CODE HERE ###"
      ],
      "metadata": {
        "id": "yY8f_1_rFJSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result"
      ],
      "metadata": {
        "id": "H2dZ-dvANM1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Show metrics and score!\"\"\"\n",
        "print(classification_report(y_test, y_predict))\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSBVMGRAMUX-",
        "outputId": "61cbdeed-d8d6-498a-9d6a-e37564b1a33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.93      0.99      0.96       200\n",
            "        True       0.99      0.93      0.96       200\n",
            "\n",
            "    accuracy                           0.96       400\n",
            "   macro avg       0.96      0.96      0.96       400\n",
            "weighted avg       0.96      0.96      0.96       400\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[198,   2],\n",
              "       [ 15, 185]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}